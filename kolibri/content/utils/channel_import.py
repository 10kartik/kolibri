from django.apps import apps
from kolibri.content.apps import KolibriContentConfig
from kolibri.content.models import ChannelMetadata, ContentNode, ContentTag, File, Language, License, LocalFile
from kolibri.utils.time import local_now

from .annotation import set_leaf_node_availability_from_local_file_availability
from .channels import read_channel_metadata_from_db_file
from .paths import get_content_database_file_path
from .sqlalchemybridge import Bridge, ClassNotFoundError


def delete_content_tree_and_files(channel_id):
    # Use Django ORM to ensure cascading delete:
    ContentNode.objects.filter(channel_id=channel_id).delete()


NO_VERSION = 'unversioned'

CONTENT_APP_NAME = KolibriContentConfig.label

merge_models = [
    ContentTag,
    LocalFile,
    License,
    Language,
]

class ChannelImport(object):

    # Specific instructions and exceptions for importing table from previous versions of Kolibri
    # The value for a particular key can either be a function, which will be invoked on the context
    # Or, as a shortcut, a string can be used that will be used to get a different attribute from the
    # source object
    # Mappings can be 'per_row', specifying mappings for an entire row
    # and 'per_table' mapping an entire table at a time. Both can be used simultaneously.
    schema_mapping = {}

    def __init__(self, channel_id, source=None):
        self.channel_id = channel_id
        self.destination = Bridge()
        # Reuse an existing source session and engine if possible
        if not source:
            source = Bridge(sqlite_file_path=get_content_database_file_path(channel_id))

        self.source = source

        self.destination = Bridge(app_name=CONTENT_APP_NAME)

        content_app = apps.get_app_config(CONTENT_APP_NAME)

        # Use this rather than get_models, as it returns a list of all models, including those
        # generated by ManyToMany fields, whereas get_models only returns explicitly defined
        # Model classes
        self.content_models = list(content_app.models.values())

        # Get the next available tree_id in our database
        self.tree_id = self.find_unique_tree_id()

    def find_unique_tree_id(self):
        ContentNodeRecord = self.destination.get_class(ContentNode)
        tree_ids = sorted(map(lambda x: x[0], self.destination.session.query(
            ContentNodeRecord.tree_id).distinct().all()))
        # If there are no pre-existing tree_ids just escape here and return 1
        if not tree_ids:
            return 1
        if len(tree_ids) == 1:
            return tree_ids[0] + 1

        # Do a binary search to find the lowest unused tree_id
        def find_hole_in_list(ids):
            last = len(ids) - 1
            middle = int(last/2 + 1)
            # Check if the lower half of ids has a hole in it
            if ids[middle] - ids[0] != middle:
                # List is only two ids, so hole must be between them
                if middle == 1:
                    return ids[0] + 1
                return find_hole_in_list(ids[:middle])
            # Otherwise check if there is a hole in the second half
            if ids[last] - ids[middle] != last - middle:
                # Second half is only two ids so hole must be between them
                if last - middle == 1:
                    return ids[middle] + 1
                return find_hole_in_list(ids[middle:])
            # We should only reach this point in the first iteration, if there are no holes in either
            # the first or the last half of the list, therefore, we just take the max of the list plus 1
            # Because the list is already sorted, we can just take the last value
            return ids[-1] + 1
        return find_hole_in_list(tree_ids)

    def generate_row_mapper(self, mappings=None):
        if mappings is None:
            mappings = {}

        def mapper(record, column):
            if column in mappings:
                col_map = mappings.get(column)
                if hasattr(record, col_map):
                    return getattr(record, col_map)
                elif hasattr(self, col_map):
                    return getattr(self, col_map)(record)
                else:
                    raise AttributeError('Column mapping specified but no valid column name or method found')
            else:
                return getattr(record, column, None)
        return mapper

    def base_table_mapper(self, SourceRecord):
        for record in self.source.session.query(SourceRecord).all():
            yield record

    def generate_table_mapper(self, mapping=None):
        if mapping is None:
            return self.base_table_mapper
        # Can only be a method on the Import object
        if hasattr(self, mapping):
            return getattr(self, mapping)
        raise AttributeError('Table mapping specified but no valid method found')

    def table_import(self, model, row_mapper, table_mapper, unflushed_rows):
        DestinationRecord = self.destination.get_class(model)
        dest_table = DestinationRecord.__table__

        # If the source class does not exist (i.e. this table is undefined in the source database)
        # this will raise an error so we set it to None. In this case, a custom table mapper must
        # have been set up to handle the fact that this is None.
        try:
            SourceRecord = self.source.get_class(model)
        except ClassNotFoundError:
            SourceRecord = None

        columns = dest_table.columns.keys()
        data_to_insert = []
        merge = model in merge_models
        for record in table_mapper(SourceRecord):
            data = {
                str(column): row_mapper(record, column) for column in columns if row_mapper(record, column) is not None
            }
            if merge:
                # Models that should be merged (see list above) need to be individually merged into the session
                # as SQL Alchemy ORM does not support INSERT ... ON DUPLICATE KEY UPDATE style queries,
                # as not available in SQLite, only MySQL as far as I can tell:
                # http://hackthology.com/how-to-compile-mysqls-on-duplicate-key-update-in-sql-alchemy.html
                self.destination.session.merge(DestinationRecord(**data))
            else:
                data_to_insert.append(data)
            unflushed_rows += 1
            if unflushed_rows == 10000:
                if not merge:
                    self.destination.session.bulk_insert_mappings(DestinationRecord, data_to_insert)
                    data_to_insert = []
                self.destination.session.flush()
                unflushed_rows = 0
        if not merge and data_to_insert:
            self.destination.session.bulk_insert_mappings(DestinationRecord, data_to_insert)
        return unflushed_rows

    def import_channel_data(self):

        unflushed_rows = 0

        for model in self.content_models:
            mapping = self.schema_mapping.get(model, {})
            row_mapper = self.generate_row_mapper(mapping.get('per_row'))
            table_mapper = self.generate_table_mapper(mapping.get('per_table'))
            unflushed_rows = self.table_import(model, row_mapper, table_mapper, unflushed_rows)
        self.destination.session.commit()

    def delete_content_tree_and_files(self):
        delete_content_tree_and_files(self.channel_id)

    def end(self):
        self.source.end()
        self.destination.end()


class NoVersionChannelImport(ChannelImport):

    schema_mapping = {
        ContentNode: {
            'per_row': {
                'channel_id': 'infer_channel_id_from_source',
                'tree_id': 'get_tree_id',
                'available': 'none',
            },
        },
        File: {
            'per_row': {
                File._meta.get_field('local_file').attname: 'checksum',
                'available': 'none',
            },
        },
        LocalFile: {
            'per_table': 'generate_local_file_from_file',
            'per_row': {
                'id': 'checksum',
                'extension': 'extension',
                'file_size': 'file_size',
                'available': 'none',
            },
        },
        ChannelMetadata: {
            'per_row': {
                ChannelMetadata._meta.get_field('min_kolibri_version').attname: 'set_version_to_no_version',
                'root_id': 'root_pk',
            },
        },
    }

    def none(self, source_object):
        return None

    def infer_channel_id_from_source(self, source_object):
        return self.channel_id

    def get_tree_id(self, source_object):
        return self.tree_id

    def generate_local_file_from_file(self, SourceRecord):
        SourceRecord = self.source.get_class(File)
        checksum_record = set()
        # LocalFile objects are unique per checksum
        for record in self.source.session.query(SourceRecord).all():
            if record.checksum not in checksum_record:
                checksum_record.add(record.checksum)
                yield record
            else:
                continue

    def set_version_to_no_version(self, source_object):
        return NO_VERSION


mappings = {
    NO_VERSION: NoVersionChannelImport
}


def initialize_import_manager(channel_id):

    channel_metadata = read_channel_metadata_from_db_file(get_content_database_file_path(channel_id))

    min_version = getattr(channel_metadata, 'min_kolibri_version', NO_VERSION)

    ImportClass = mappings.get(min_version)

    return ImportClass(channel_id)


def import_channel_from_local_db(channel_id):
    import_manager = initialize_import_manager(channel_id)

    if ChannelMetadata.objects.filter(id=channel_id).exists():
        # We have already imported this channel in some way, so let's clean up first.
        import_manager.delete_content_tree_and_files()

    import_manager.import_channel_data()

    import_manager.end()

    set_leaf_node_availability_from_local_file_availability()

    channel = ChannelMetadata.objects.get(id=channel_id)
    channel.last_updated = local_now()
    channel.save()
